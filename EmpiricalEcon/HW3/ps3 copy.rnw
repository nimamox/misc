%Problem Set 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Sweave('C:/Klaus/AAEC5126/problemsets/ps3',syntax=SweaveSyntaxNoweb)

%I) DEFINE DOCUMENTCLASS AND LOAD ALL REQUIRED PACKAGES
\documentclass[11pt,reqno]{amsart}   %keep it simple
\usepackage[x11names]{xcolor}
\usepackage{fancyvrb}
\DefineVerbatimEnvironment{Sinput}{Verbatim}{formatcom=\color{Blue3}}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{formatcom=\color{gray}}
\usepackage{graphicx}      % for fancy graphics
\usepackage{setspace}      % for basic formatting
\usepackage{enumerate}     % for more flexibility with numbered lists
\SweaveOpts{keep.source=TRUE}  %KEY - this preserves R formatting and comments

% You may need to load all or some of these packages -
%follow the instructions on our course web site under "Help with LaTex"

%II) PREAMBLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{plain} %puts page number center bottom
\setlength{\topmargin}{0in}
\setlength{\textheight}{8.5in}
\setlength{\oddsidemargin}{.0in}
\setlength{\evensidemargin}{.0in}
\setlength{\textwidth}{6.5in}
\setlength{\footskip}{.5in}
\setlength{\parindent}{0in} %suppress indentation
%\onehalfspacing

\newcommand{\mlt}[1]{\mathbf{#1}} %matrix bold for Latin symbols
\newcommand{\mgr}[1]{\boldsymbol{#1}}%matrix bold for Greek symbols
\newcommand{\kR}{\tt R\rm{} }%shortcut for "R" symbol
\newcommand{\ksp}{\vspace{0.1in}}   % insert some space between chunks
%feel free to add your own shortcuts  - here a mine:
\newcommand{\kl}{\left(}
\newcommand{\kr}{\right)}
\newcommand{\kll}{\left\{}
\newcommand{\krr}{\right\}}
\newcommand{\kmu}{\mgr{\mu}}
\newcommand{\kpsi}{\mgr{\psi}}
\newcommand{\kphi}{\mgr{\phi}}
\newcommand{\kgam}{\mgr{\gamma}}
\newcommand{\ktheta}{\mgr{\theta}}
\newcommand{\kbeta}{\mgr{\beta}}
\newcommand{\kdelta}{\mgr{\delta}}
\newcommand{\kt}{^{\prime}}
\newcommand{\kdel}{\partial}
\newcommand{\kdot}{\kl . \kr}
\newcommand{\keps}{\epsilon}
\newcommand{\kx}{\mlt{x}}
\newcommand{\kX}{\mlt{X}}
\newcommand{\kV}{\mlt{V}}
\newcommand{\kM}{\mlt{M}}
\newcommand{\kP}{\mlt{P}}
\newcommand{\ky}{\mlt{y}}
\newcommand{\kb}{\mlt{b}}
\newcommand{\kc}{\mlt{c}}
\newcommand{\ki}{\mlt{i}}
\newcommand{\ke}{\mlt{e}}
\newcommand{\klam}{\lambda}
\newcommand{\kp}{\mlt{p}}
\newcommand{\kprob}{\text{prob}}
\newcommand{\kz}{\mlt{z}}
\newcommand{\ksig}{\sigma^2}
\newcommand{\kSig}{\mgr{\Sigma}}
\newcommand{\klog}{\text{log}}
\newcommand{\kols}{\kl \kX\kt\kX\kr^{-1}\kX\kt\ky}
\newcommand{\kSSE}{\kl \ky-\kX\kb\kr\kt\kl\ky-\kX\kb\kr}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\SweaveOpts{concordance=TRUE}
%%%%%%%%%%%%%%%%%%%%%%%%

%III) TOP MATTER INFORMATION
\title{Problem Set 3}
\author{} %ENTER YOUR NAME HERE
\maketitle %this comes at the end of the top matter to set it.

<<R1,echo=FALSE>>=
options(prompt = "R> ", digits = 4)
options(continue=" ") 
setwd('/Users/nima/AAEC5126/HW3/')
<<echo=FALSE>>=
options(continue=" ")
options(width=60)
set.seed(37)  
library("xtable")
library(knitr)
render_sweave()
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{MLE Bernoulli}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Consider the Bernoulli model (also called "Binary response model") where a given random draw $y_i = 1$ with probability $\pi$, 
and $y_i = 0$ with probability $1-\pi$.  
Thus, the density for this model can be compactly written as $f\kl y_i \kr =\pi^{y_i}\kl 1-\pi \kr^{1-y_i}$.  
The moments for this density are given as $E\kl y_i \kr = \pi, V\kl y_i \kr =\pi\kl 1-\pi \kr$. 
Assume you draw a sample of $n$ observations from this distribution.
\ksp
\begin{enumerate}
\item  Derive $lnL\kl \pi \kr, g\kl \pi \kr, H\kl \pi \kr, I\kl \pi \kr$.

$f\kl y_i \kr =\pi^{y_i}\kl 1-\pi \kr^{1-y_i}$\\
$\Rightarrow l\kl \pi \kr =\pi^{y_i}\kl 1-\pi \kr^{1-y_i}$\\
$\Rightarrow \ln l\kl \pi \kr = y_i \ln \kl \pi \kr + \kl 1-y_i\kr \ln\kl 1-\pi \kr$\\

Then, summing over all observations we have:
\ksp

\begin{equation*}
\begin{split}
\ln \mathrm{L}(\pi)&=\ln (\pi) \sum_{i=1}^{n} y_{i}+\ln (1-\pi) \sum_{i=1}^{n}\left(1-y_{i}\right)\\
g(\pi)&=\frac{\partial \ln L(\pi)}{\partial \pi}=\frac{1}{\pi} \sum_{i=1}^{n} y_{i}-\frac{1}{1-\pi} \sum_{i=1}^{n}\left(1-y_{i}\right)\\
H(\pi)&=\frac{\partial^{2} \ln L(\pi)}{\partial \pi^{2}}=-\frac{1}{\pi^{2}} \sum_{i=1}^{n} y_{i}-\frac{1}{(1-\pi)^{2}} \sum_{i=1}^{n}\left(1-y_{i}\right)\\
I(\pi)&=-\mathrm{E}_{y}(H(\pi))=-\mathrm{E}_{y}\left(-\frac{1}{\pi^{2}} \sum_{i=1}^{n} y_{i}-\frac{1}{(1-\pi)^{2}} \sum_{i=1}^{n}\left(1-y_{i}\right)\right)\\&=\frac{n \pi}{\pi^{2}}+\frac{n(1-\pi)}{(1-\pi)^{2}}=\frac{n}{\pi(1-\pi)}
\end{split}
\end{equation*}



\item	Derive the ML estimator (call it $p$), and its asymptotic variance.

\begin{equation*}
\begin{split}
& g(\pi)=\frac{1}{\pi} \sum_{i=1}^{n} y_{i}-\frac{1}{1-\pi} \sum_{i=1}^{n}\left(1-y_{i}\right)=0\\
& \Rightarrow \frac{1}{\pi} \sum_{i=1}^{n} y_{i}=\frac{1}{1-\pi} \sum_{i=1}^{n}\left(1-y_{i}\right) \Rightarrow \frac{1}{\pi} \sum_{i=1}^{n} y_{i}=\frac{n}{1-\pi}-\frac{1}{1-\pi} \sum_{i=1}^{n} y_{i} \\
& \Rightarrow \frac{1}{\pi} \sum_{i=1}^{n} y_{i}+\frac{1}{1-\pi} \sum_{i=1}^{n} y_{i}=\frac{n}{1-\pi} \Rightarrow \left(\frac{1}{\pi}+\frac{1}{1-\pi}\right) \sum_{i=1}^{n} y_{i}=\frac{n}{1-\pi}\\
&\Rightarrow \sum_{i=1}^{n} y_{i} = n\pi\\
&\Rightarrow p=\frac{\sum_{i=1}^{n} y_{i}}{n}
\end{split}
\end{equation*}

$Var(p) =[I(\pi)]^{-1} =[\dfrac{n}{\pi(1-\pi)}]^{-1} = \dfrac{\pi(1-\pi)}{n}$

\item	Given a sample of four "1"'s and one "0" (so n=5), compute $lnL\kl \pi \kr$ in terms of $\pi$ and numerically.

Our estimator gives
$$p=\frac{\sum_{i=1}^{n} y_{i}}{n}=\frac{4}{5}=0.8$$

We have
\begin{equation*}
\begin{split}
f(y_i = 1) &= \pi\\
f(y_i = 0) &= (1 - \pi)
\end{split}
\end{equation*}

Then the likelihood function is evaluated as
\begin{equation*}
\begin{split}
L(\pi)=\pi^{4}(1-\pi) \Rightarrow \ln L(\pi)=4 \ln (\pi)+\ln (1-\pi)
\end{split}
\end{equation*}

Then

$$\ln L(p)=4 \ln (.8)+\ln (1-.8)=-2.502$$

\item	Now suppose that in the sample of 5 observations, all are "1's".  
Derive the numerical solution for the ML estimator for this case. 

\begin{equation*}
\begin{split}
& p=\frac{\sum_{i=1}^{n} y_{i}}{n}=\frac{5}{5}=1\\
\Rightarrow & \ln L(\pi)=5 \ln (\pi)\\
\Rightarrow & \ln L(p)=5 \ln (1)=0
\end{split}
\end{equation*}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{MLE Parameterized Exponential}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Consider the Exponential density with parameterized mean, i.e
\begin{equation*}
\begin{split}
&f\kl y_i |\mlt{x}_i\kr = \frac{exp\kl -y_i /\mlt{x}_i\kt\mgr{\beta}\kr}{\mlt{x}_i\kt\mgr{\beta}} \quad \text{with} \\
&E\kl y_i |\mlt{x}_i\kr= \mlt{x}_i\kt\mgr{\beta}, \quad V\kl y_i|\mlt{x}_i\kr = \kl  \mlt{x}_i\kt\mgr{\beta}\kr^2
\end{split}
\end{equation*}
\ksp

Assume the sample size is $n$ and $\mlt{x}_i$ is $k \times 1$.
\ksp

\begin{enumerate}
\item Derive $lnL\kl \mgr{\beta}\kr, g \kl \mgr{\beta}\kr,H\kl \mgr{\beta}\kr$ and $I\kl \mgr{\beta}\kr$

$$l(\boldsymbol{\beta})=\frac{\exp \left(-y_{i} / \mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)}{\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}}$$\\
$$\ln l(\boldsymbol{\beta})=-\frac{y_{i}}{\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}}-\ln \left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)$$

\begin{equation*}
\begin{split}
\ln L(\boldsymbol{\beta})&=-\sum_{i=1}^{n} y_{i}\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-1}-\sum_{i=1}^{n} \ln \left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)\\
g(\boldsymbol{\beta})&=\frac{\partial \ln L(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}=\sum_{i=1}^{n}\left[y_{i}\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-2} \mathbf{x}_{i}\right]-\sum_{i=1}^{n}\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-1} \mathbf{x}_{\mathbf{i}}\\
H(\boldsymbol{\beta})&=\frac{\partial g(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}^{\prime}}=-2 \sum_{i=1}^{n}\left[y_{i}\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-3} \mathbf{x}_{i} \mathbf{x}_{i}^{\prime}\right]+\sum_{i=1}^{n}\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-2} \mathbf{x}_{i} \mathbf{x}_{i}^{\prime}\\
I(\boldsymbol{\beta})&=-E_{y}[H(\boldsymbol{\beta})]=2 \sum_{i=1}^{n} E\left(y_{i}\right)\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-3} \mathbf{x}_{i} \mathbf{x}_{i}^{\prime}-\sum_{i=1}^{n}\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-2} \mathbf{x}_{i} \mathbf{x}_{i}^{\prime}\\
&=2 \sum_{i=1}^{n}\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-3} \mathbf{x}_{i} \mathbf{x}_{i}^{\prime}-\sum_{i=1}^{n}\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-2} \mathbf{x}_{i} \mathbf{x}_{i}^{\prime}
=2 \sum_{i=1}^{n}\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-2} \mathbf{x}_{i} \mathbf{x}_{i}^{\prime}-\sum_{i=1}^{n}\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-2} \mathbf{x}_{i} \mathbf{x}_{i}^{\prime}\\
& = \sum_{i=1}^{n}\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-2} \mathbf{x}_{i} \mathbf{x}_{i}^{\prime}
\end{split}
\end{equation*}

\item Using the gradient for the entire sample, show that the score identity holds.

\begin{equation*}
\begin{split}
E_{y}[g(\boldsymbol{\beta})]&=\sum_{i=1}^{n}\left[E\left(y_{i} | \mathbf{x}_{i}\right)\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-2} \mathbf{x}_{i}-\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-1} \mathbf{x}_{\mathbf{i}}\right] \\
&= \sum_{i=1}^{n}\left[\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-1} \mathbf{x}_{i}-\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-1} \mathbf{x}_{i}\right]=0
\end{split}
\end{equation*}

\item Using the gradient for the entire sample, show that the information matrix identity holds.

\begin{equation*}
\begin{split}
V[g(\boldsymbol{\beta}) | \mathbf{X}]&=\sum_{\Sigma} V\left(y_{i} | \mathbf{x}_{i}\right)\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-4} \mathbf{x}_{i} \mathbf{x}_{i}^{\prime}\\
&=\sum_{i=1}^{n}\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{2}\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-4} \mathbf{x}_{i} \mathbf{x}_{i}^{\prime} = \sum_{i=1}^{n}\left(\mathbf{x}_{i}^{\prime} \boldsymbol{\beta}\right)^{-2} \mathbf{x}_{i} \mathbf{x}_{i}^{\prime}\\& = I(\boldsymbol{\beta})
\end{split}
\end{equation*}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hypothesis Testing in MLE}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Consider the hedonic property value data set from script \texttt{mod3s2c} and 
the log-linear regression version of this model.  Estimate this model via MLE,  
using analytical gradient and Hessian (as in \texttt{mod2s1b}). 
Use the inverted negative Hessian to derive standard errors for all estimates. 
Capture your MLE results in a nice table.\\

Loading data:
<<R2,results=hide,echo=TRUE>>=
data<- read.table('/Users/nima/AAEC5126/data/hedonics.txt', sep="\t", header=FALSE)

colnames(data) <- c("price", "lnacres", "lnsqft", "age","gradeab", "pkadeq",
                   "vacant", "empden", "popden", "metro", "distair", "disthaz")

attach(data)
n <- nrow(data)
X <- cbind(rep(1, n), data[,-1])
colnames(X)[1] <- "const"
k <- ncol(X)
y <- log(data$price)
@

\ksp
Function for optimization components (llf, g, H):

<<R3,results=hide,echo=TRUE>>=
CLRMllfan<-function(x, y, X, n, k){
    bm <- x[1:k]
    sig2 <- x[k + 1]^2  #square to keep positive
    
    llf <- -(n/2) * log(2 * pi) - (n/2) * log(sig2) - ((1/(2 *
        sig2)) * t(y - X %*% bm) %*% (y - X %*% bm))
    # sample log-lh for the CLRM
    
    # Gradient
    g1 <- (t(X) %*% (y - X %*% bm))/sig2
    g2 <- -(n/(2 * sig2)) + ((t(y - X %*% bm) %*% (y -
        X %*% bm))/((2 * sig2^2)))
    g <- rbind(g1, g2)
    
    # Hessian
    H1 <- -(t(X) %*% X)/sig2
    H2 <- -(t(X) %*% (y - X %*% bm))/(sig2^2)
    H3 <- t(H2)
    H4 <- n/(2 * sig2^2) - (t(y - X %*% bm) %*% (y - X %*%
        bm)/sig2^3)
    H <- rbind(cbind(H1, H2), cbind(H3, H4))
    return(list(llf, g, H))
}
@

Choose the following tuner settings for the optimization algorithm:\\

\ksp
<<R2,results=hide,echo=TRUE>>=
cri<-10        #initial setting for convergence criterion
cri1<-0.0001    #convergence criterion 
#(here for the sum of the absolute values of the elements in the gradient)
maxiter<-2000   #max. number of allowed iterations
stsz<-0.1      #step size, here it seems necessary to keep it on the small side 
#... try 0.5 and see what happens....
@
\ksp
\begin{enumerate}[(1)]
\item Which coefficients are significant at the 1\% or 5\% level?
\item Interpret the marginal effects of "lnacres" , "gradeab", and "disthaz" on the dependent variable.
\item Derive an estimate for the \emph{variance} of the error term, along with a 95\% confidence interval.
\item Perform Wald tests for the following hypotheses. For each hypothesis obtain the test statistic and the corresponding p-value, and state your decision (use script \texttt{mod3s2b} for guidance on Wald tests in an MLE context):\\

    \begin{enumerate} [(a)]
    \item The marginal effects of "age" ($\beta_4$), "pkadeq" ($\beta_6$), "vacant" ($\beta_7$) and "popden" ($\beta_9$) are jointly zero.
    \item The effect of an additional mile away from the airport on the value of a property 
is LESS OR EQUAL TO 3 times that of an additional mile from the hazardous waste site.\\
(\emph{Hint: There are multiple ways to set up the null hypothesis. Choose the one that has a zero on the right hand side of the equation.})
    \item The ratio of (additional mile from airport / additional mile from haz. waste site) 
is NO SMALLER THAN the squared effect of "log of square footage".\\
(\emph{Hint: There are multiple ways to set up the null hypothesis. Choose the one that has a zero on the right hand side of the equation.})
    \end{enumerate}
    
\end{enumerate}




\end{document}

 

